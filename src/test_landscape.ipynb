{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "style.use('seaborn')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from test import *\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, CNNCifar_fedVC, CNNCifar_VCBN, CNNCifar_VCGN,CNNCifar_WS\n",
    "from utils import get_dataset, average_weights, exp_details, get_logger, check_norm\n",
    "\n",
    "from resnet_gn import resnet18\n",
    "from resnet import ResNet32_test\n",
    "from vgg import vgg11_bn,vgg11\n",
    "import  random\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "print('check package ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our package dir to path \n",
    "module_path = os.path.dirname(os.getcwd())\n",
    "sys.path.append(module_path)\n",
    "\n",
    "home_path = module_path\n",
    "figures_path = os.path.join(home_path, 'reports', 'figures')\n",
    "models_path = os.path.join(home_path, 'reports', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device_id)\n",
    "\n",
    "device = torch.device(\"cuda:{}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(device_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_loss_average = [] # T round마다의 loss -> 학습 확인.\n",
    "net_acc_average = []  # T round마다의 acc  -> 학습 확인.\n",
    "net_loss_client = []  # client마다 전체 loss과정 추적.\n",
    "net_conv_grad = []    # client마다 전체 conv grad과정 추적.\n",
    "net_fc_grad = []      # client마다 전체 fc grad 과정 추적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "local_ep = 10\n",
    "num_users= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 fedavg, lr 0.1\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'nothing',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "print(args)\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "net_loss_average.append(train_loss)\n",
    "net_acc_average.append(train_accuracy)\n",
    "net_loss_client.append(client_loss)\n",
    "net_conv_grad.append(client_conv_grad)\n",
    "net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 fedavg, lr 0.01\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'nothing',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "net_loss_average.append(train_loss)\n",
    "net_acc_average.append(train_accuracy)\n",
    "net_loss_client.append(client_loss)\n",
    "net_conv_grad.append(client_conv_grad)\n",
    "net_fc_grad.append(client_fc_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 fedavg, lr 0.001\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'nothing',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.001,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "net_loss_average.append(train_loss)\n",
    "net_acc_average.append(train_accuracy)\n",
    "net_loss_client.append(client_loss)\n",
    "net_conv_grad.append(client_conv_grad)\n",
    "net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 fedavg, lr 0.0001\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'nothing',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.0001,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "net_loss_average.append(train_loss)\n",
    "net_acc_average.append(train_accuracy)\n",
    "net_loss_client.append(client_loss)\n",
    "net_conv_grad.append(client_conv_grad)\n",
    "net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_net_loss_average = [] # T round마다의 loss -> 학습 확인.\n",
    "bn_net_acc_average = []  # T round마다의 acc  -> 학습 확인.\n",
    "bn_net_loss_client = []  # client마다 전체 loss과정 추적.\n",
    "bn_net_conv_grad = []    # client마다 전체 conv grad과정 추적.\n",
    "bn_net_fc_grad = []      # client마다 전체 fc grad 과정 추적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg bn lr 0.1\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'batch_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "bn_net_loss_average.append(train_loss)\n",
    "bn_net_acc_average.append(train_accuracy)\n",
    "bn_net_loss_client.append(client_loss)\n",
    "bn_net_conv_grad.append(client_conv_grad)\n",
    "bn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg bn lr 0.01\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'batch_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "bn_net_loss_average.append(train_loss)\n",
    "bn_net_acc_average.append(train_accuracy)\n",
    "bn_net_loss_client.append(client_loss)\n",
    "bn_net_conv_grad.append(client_conv_grad)\n",
    "bn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg bn lr 0.001\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'batch_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.001,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "bn_net_loss_average.append(train_loss)\n",
    "bn_net_acc_average.append(train_accuracy)\n",
    "bn_net_loss_client.append(client_loss)\n",
    "bn_net_conv_grad.append(client_conv_grad)\n",
    "bn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg bn lr 0.0001\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'batch_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.0001,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "bn_net_loss_average.append(train_loss)\n",
    "bn_net_acc_average.append(train_accuracy)\n",
    "bn_net_loss_client.append(client_loss)\n",
    "bn_net_conv_grad.append(client_conv_grad)\n",
    "bn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_net_loss_average = [] # T round마다의 loss -> 학습 확인.\n",
    "gn_net_acc_average = []  # T round마다의 acc  -> 학습 확인.\n",
    "gn_net_loss_client = []  # client마다 전체 loss과정 추적.\n",
    "gn_net_conv_grad = []    # client마다 전체 conv grad과정 추적.\n",
    "gn_net_fc_grad = []      # client마다 전체 fc grad 과정 추적."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg gn lr 0.1\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'group_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "gn_net_loss_average.append(train_loss)\n",
    "gn_net_acc_average.append(train_accuracy)\n",
    "gn_net_loss_client.append(client_loss)\n",
    "gn_net_conv_grad.append(client_conv_grad)\n",
    "gn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg gn lr 0.01\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'group_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': local_ep,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "gn_net_loss_average.append(train_loss)\n",
    "gn_net_acc_average.append(train_accuracy)\n",
    "gn_net_loss_client.append(client_loss)\n",
    "gn_net_conv_grad.append(client_conv_grad)\n",
    "gn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg gn lr 0.001\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'only_one_class',\n",
    "    'seed': 0,\n",
    "    'norm': 'group_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': 10,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.001,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "gn_net_loss_average.append(train_loss)\n",
    "gn_net_acc_average.append(train_accuracy)\n",
    "gn_net_loss_client.append(client_loss)\n",
    "gn_net_conv_grad.append(client_conv_grad)\n",
    "gn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fedavg gn lr 0.0001\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"model\": 'cnn_ws',\n",
    "    'dataset': 'cifar',\n",
    "    'gpu': 0,\n",
    "    'iid': 2,\n",
    "    'epochs': epochs,\n",
    "    'optimizer': 'clip',\n",
    "    'seed': 0,\n",
    "    'norm': 'group_norm',\n",
    "    'num_users': num_users,\n",
    "    'frac': 1,\n",
    "    'local_ep': 10,\n",
    "    'local_bs': 64,\n",
    "    'lr': 0.0001,\n",
    "    'momentum': 0.1,\n",
    "    'kernel_num': 9,\n",
    "    'kernel_sizes': '3,4,5',\n",
    "    'num_channnels': '1',\n",
    "    'num_filters': 32,\n",
    "    'max_pool': 'True',\n",
    "    'num_classes': 10,\n",
    "    'unequal': 0,\n",
    "    'stopping_rounds': 10,\n",
    "    'verbose': 1,\n",
    "\n",
    "})\n",
    "\n",
    "train_loss, train_accuracy, client_loss, client_conv_grad, client_fc_grad = main_test(args)\n",
    "\n",
    "gn_net_loss_average.append(train_loss)\n",
    "gn_net_acc_average.append(train_accuracy)\n",
    "gn_net_loss_client.append(client_loss)\n",
    "gn_net_conv_grad.append(client_conv_grad)\n",
    "gn_net_fc_grad.append(client_fc_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 no norm learning curve\n",
    "plt.figure()\n",
    "\n",
    "plt.title('traing loss')\n",
    "plt.plot(range(len(net_loss_average[2])), net_loss_average[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools -> 기본 \n",
    "for loss_landscape in net_loss_client:\n",
    "    for idx in range(num_users):\n",
    "        \n",
    "loss_1 = np.array(loss_1).flatten()\n",
    "loss_01 = np.array(loss_01).flatten()\n",
    "loss_001 = np.array(loss_001).flatten()\n",
    "loss_0001 = np.array(loss_0001).flatten()\n",
    "min_curve = []\n",
    "max_curve = []\n",
    "for i in range(len(loss_1)):\n",
    "    min_curve.append(np.min([loss_1[i], loss_01[i], loss_001[i], loss_0001[i]]))\n",
    "    max_curve.append(np.max([loss_1[i], loss_01[i], loss_001[i], loss_0001[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-hartford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.fill_between(range(len(min_curve)), min_curve, max_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools -> bn\n",
    "loss_bn_1 = np.array(loss_bn_1).flatten()\n",
    "loss_bn_01 = np.array(loss_bn_01).flatten()\n",
    "loss_bn_001 = np.array(loss_bn_001).flatten()\n",
    "loss_bn_0001 = np.array(loss_bn_0001).flatten()\n",
    "min_curve = []\n",
    "max_curve = []\n",
    "for i in range(len(loss_bn_1)):\n",
    "    min_curve_batch.append(np.min([loss_bn_1[i], loss_bn_01[i], loss_bn_001[i], loss_bn_0001[i]]))\n",
    "    max_curve_batch.append(np.max([loss_bn_1[i], loss_bn_01[i], loss_bn_001[i], loss_bn_0001[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 + bn\n",
    "step = 30\n",
    "steps = np.arange(0, len(min_curve), step)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.fill_between(steps, min_curve[::step], max_curve[::step],\n",
    "                alpha=0.5, color='C1', label='Standart CNN')\n",
    "plt.plot(steps, min_curve[::step], color='C1')\n",
    "plt.plot(steps, max_curve[::step], color='C1')\n",
    "\n",
    "plt.fill_between(steps, min_curve_batch[::step], max_curve_batch[::step],\n",
    "                alpha=0.5, color='C2', label='Standart CNN + BatchNorm')\n",
    "plt.plot(steps, min_curve_batch[::step], color='C2')\n",
    "plt.plot(steps, max_curve_batch[::step], color='C2')\n",
    "\n",
    "plt.legend(fontsize=19)\n",
    "plt.title('Loss Landscape', fontsize=20)\n",
    "plt.ylabel('Loss Landscape', fontsize=13)\n",
    "plt.xlabel('Steps', fontsize=13)\n",
    "plt.savefig(os.path.join(figures_path, 'loss_landscape.png'), dpi=500, quality=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient - import itertools\n",
    "lol_01 = np.array(grads_01).flatten()\n",
    "lol_02 = np.array(grads_02).flatten()\n",
    "lol_005 = np.array(grads_005).flatten()\n",
    "lol_001 = np.array(grads_001).flatten()\n",
    "\n",
    "kek_01 = []\n",
    "kek_02 = []\n",
    "kek_005 = []\n",
    "kek_001 = []\n",
    "for i in range(1,len(lol_01)):\n",
    "  kek_01.append((lol_01[i-1] - lol_01[i]).norm(p=2).item())\n",
    "  kek_02.append((lol_02[i-1] - lol_02[i]).norm(p=2).item())\n",
    "  kek_005.append((lol_005[i-1] - lol_005[i]).norm(p=2).item())\n",
    "  kek_001.append((lol_001[i-1] - lol_001[i]).norm(p=2).item())\n",
    "  \n",
    "min_curve = []\n",
    "max_curve = []\n",
    "for i in range(len(kek_01)):\n",
    "    min_curve.append(np.min([kek_01[i], kek_02[i], kek_005[i], kek_001[i]]))\n",
    "    max_curve.append(np.max([kek_01[i], kek_02[i], kek_005[i], kek_001[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient - import itertools\n",
    "lol_01 = np.array(grads_01).flatten()\n",
    "lol_02 = np.array(grads_02).flatten()\n",
    "lol_005 = np.array(grads_005).flatten()\n",
    "lol_001 = np.array(grads_001).flatten()\n",
    "\n",
    "kek_01 = []\n",
    "kek_02 = []\n",
    "kek_005 = []\n",
    "kek_001 = []\n",
    "for i in range(1,len(lol_01)):\n",
    "  kek_01.append((lol_01[i-1] - lol_01[i]).norm(p=2).item())\n",
    "  kek_02.append((lol_02[i-1] - lol_02[i]).norm(p=2).item())\n",
    "  kek_005.append((lol_005[i-1] - lol_005[i]).norm(p=2).item())\n",
    "  kek_001.append((lol_001[i-1] - lol_001[i]).norm(p=2).item())\n",
    "  \n",
    "min_curve = []\n",
    "max_curve = []\n",
    "for i in range(len(kek_01)):\n",
    "    min_curve.append(np.min([kek_01[i], kek_02[i], kek_005[i], kek_001[i]]))\n",
    "    max_curve.append(np.max([kek_01[i], kek_02[i], kek_005[i], kek_001[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
